import os, json, glob, re, time
import numpy as np
import faiss
from typing import List, Tuple, Dict
from .llm import embed_texts

VSTORE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "vectorstore")
CORPUS_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "ingest", "corpus")

CHUNK_SIZE = 700
CHUNK_OVERLAP = 100
TOP_K = int(os.getenv("TOP_K","6"))

def _split_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    chunks = []
    start = 0
    
    # ç¡®ä¿overlapå°äºchunk_sizeï¼Œé¿å…æ— é™å¾ªç¯
    if overlap >= chunk_size:
        overlap = chunk_size // 2
    
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunk = text[start:end]
        
        # åªæ·»åŠ éç©ºå—
        if chunk.strip():
            chunks.append(chunk)
        
        # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®
        next_start = end - overlap
        
        # é˜²æ­¢æ— é™å¾ªç¯ï¼šç¡®ä¿æ¯æ¬¡éƒ½å‘å‰æ¨è¿›
        if next_start <= start:
            next_start = start + 1
            
        start = next_start
        
        # å¦‚æœå·²ç»åˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œé€€å‡º
        if end >= len(text):
            break
            
        # é˜²æ­¢æ„å¤–çš„æ— é™å¾ªç¯ï¼ˆå®‰å…¨æªæ–½ï¼‰
        if len(chunks) > 1000:  # å‡è®¾ä¸ä¼šæœ‰è¶…è¿‡1000ä¸ªå—
            print(f"âš ï¸ è­¦å‘Šï¼šæ–‡æœ¬åˆ†å‰²äº§ç”Ÿäº†è¿‡å¤šå—æ•° ({len(chunks)})ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
            break
    
    return chunks

def _load_corpus() -> List[Dict]:
    files = glob.glob(os.path.join(CORPUS_DIR, "*.*"))
    out = []
    print(f"ğŸ“ å¤„ç†è¯­æ–™åº“æ–‡ä»¶: {len(files)} ä¸ªæ–‡ä»¶")
    
    for fp in files:
        try:
            with open(fp, "r", encoding="utf-8", errors="ignore") as f:
                txt = f.read()
            title = os.path.basename(fp)
            print(f"   ğŸ“„ å¤„ç†æ–‡ä»¶: {title}")
            
            # å°è¯•æå–æ—¥æœŸä¿¡æ¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            date = None
            # è‹±æ–‡æ ¼å¼
            m_date = re.search(r"Effective date:\s*([0-9\-]+)", txt)
            if m_date:
                date = m_date.group(1)
            else:
                # ä¸­æ–‡æ ¼å¼
                m_date = re.search(r"ç”Ÿæ•ˆæ—¥æœŸ[ï¼š:]\s*([0-9\-]+)", txt)
                if m_date:
                    date = m_date.group(1)
            
            # å°è¯•æå–æ¥æºä¿¡æ¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            src = None
            # è‹±æ–‡æ ¼å¼
            m_src = re.search(r"Source:\s*(.*)$", txt, re.MULTILINE)
            if m_src:
                src = m_src.group(1).strip()
            else:
                # ä¸­æ–‡æ ¼å¼
                m_src = re.search(r"æ¥æº[ï¼š:]\s*(.*)$", txt, re.MULTILINE)
                if m_src:
                    src = m_src.group(1).strip()
            
            chunks = _split_text(txt)
            print(f"      åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
            
            for i, ch in enumerate(chunks):
                if ch.strip():  # åªæ·»åŠ éç©ºçš„æ–‡æœ¬å—
                    out.append({
                        "title": title,
                        "date": date,
                        "url": src,
                        "chunk_id": f"{title}#chunk{i}",
                        "text": ch.strip()
                    })
        except Exception as e:
            print(f"   âŒ å¤„ç†æ–‡ä»¶ {fp} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {len(out)} ä¸ªæ–‡æ¡£å—")
    return out

def _paths():
    return (
        os.path.join(VSTORE_DIR, "index.faiss"),
        os.path.join(VSTORE_DIR, "docs.json")
    )

def build_or_load():
    faiss_path, docs_path = _paths()
    if os.path.exists(faiss_path) and os.path.exists(docs_path):
        index = faiss.read_index(faiss_path)
        with open(docs_path, "r", encoding="utf-8") as f:
            docs = json.load(f)
        return index, docs

    docs = _load_corpus()
    if not docs:
        raise ValueError("è¯­æ–™åº“ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç´¢å¼•")
    
    texts = [d["text"] for d in docs]
    
    # æ‰¹é‡å¤„ç†åµŒå…¥ä»¥èŠ‚çœå†…å­˜å’Œæé«˜æ•ˆç‡
    print(f"ğŸ”§ å¼€å§‹ç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬å—çš„åµŒå…¥å‘é‡...")
    batch_size = 5  # å¢åŠ æ‰¹å¤„ç†å¤§å°æé«˜æ•ˆç‡
    all_embs = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_num = i//batch_size + 1
        total_batches = (len(texts)-1)//batch_size + 1
        
        print(f"   ğŸ”„ æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç† {len(batch_texts)} ä¸ªæ–‡æœ¬å—...")
        
        try:
            start_time = time.time()
            batch_embs = embed_texts(batch_texts)
            elapsed = time.time() - start_time
            print(f"      âœ… å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            
            all_embs.extend(batch_embs)
            
            # é€‚å½“ä¼‘æ¯é¿å…APIé™åˆ¶
            if batch_num < total_batches:
                time.sleep(0.2)
                
        except Exception as e:
            print(f"      âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
            # å°è¯•å•ä¸ªå¤„ç†å¤±è´¥çš„æ‰¹æ¬¡
            print(f"      ğŸ”„ å°è¯•å•ä¸ªå¤„ç†...")
            for j, single_text in enumerate(batch_texts):
                try:
                    single_emb = embed_texts([single_text])
                    all_embs.extend(single_emb)
                    print(f"         âœ… å•ä¸ªæ–‡æœ¬ {j+1} å¤„ç†æˆåŠŸ")
                except Exception as e2:
                    print(f"         âŒ å•ä¸ªæ–‡æœ¬ {j+1} ä¹Ÿå¤±è´¥: {e2}")
                    # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                    if all_embs:
                        dummy_emb = [0.0] * len(all_embs[0])
                        all_embs.append(dummy_emb)
                        print(f"         âš ï¸ ä½¿ç”¨é›¶å‘é‡å ä½")
    
    if not all_embs:
        raise ValueError("æ— æ³•ç”Ÿæˆä»»ä½•åµŒå…¥å‘é‡")
    
    print(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_embs)} ä¸ª")
    
    # æ„å»ºFAISSç´¢å¼•
    print("ğŸ”§ æ„å»ºFAISSç´¢å¼•...")
    dim = len(all_embs[0])
    index = faiss.IndexFlatIP(dim)
    
    # Normalize for cosine similarity via inner product
    vecs = np.array(all_embs).astype("float32")
    # L2 normalize
    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-10
    vecs = vecs / norms
    index.add(vecs)

    # ç¡®ä¿vectorstoreç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(faiss_path), exist_ok=True)
    
    # ä¿å­˜ç´¢å¼•å’Œæ–‡æ¡£
    print("ğŸ’¾ ä¿å­˜ç´¢å¼•æ–‡ä»¶...")
    faiss_path, docs_path = _paths()
    faiss.write_index(index, faiss_path)
    with open(docs_path, "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)
    
    print("ğŸ‰ ç´¢å¼•æ„å»ºå®Œæˆï¼")
    return index, docs

def search(query: str, k: int = TOP_K) -> List[Dict]:
    index, docs = build_or_load()
    q_emb = embed_texts([query])[0]
    q = np.array([q_emb]).astype("float32")
    q = q / (np.linalg.norm(q, axis=1, keepdims=True) + 1e-10)
    D, I = index.search(q, k)
    hits = []
    for idx, score in zip(I[0], D[0]):
        if idx == -1: continue
        d = dict(docs[idx])
        d["score"] = float(score)
        hits.append(d)
    return hits
